{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0190984-26f4-4644-9a63-626ad57078d0",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) Classification Algorithm\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks. However, it is primarily used for classification. SVM aims to find a hyperplane in an N-dimensional space (where N is the number of features) that distinctly classifies the data points.\n",
    "\n",
    "SVM is highly effective in high-dimensional spaces and is still effective when the number of dimensions is greater than the number of samples. It performs well in cases where the data is not linearly separable by transforming the original features into higher-dimensional space (using a kernel trick).\n",
    "\n",
    "### Why is SVM Used?\n",
    "\n",
    "- **Effective in High Dimensions**: SVM performs well in high-dimensional spaces, making it suitable for datasets with many features.\n",
    "- **Works Well with Non-Linear Data**: By using the kernel trick, SVM can handle non-linear classification problems.\n",
    "- **Robust**: SVM is particularly effective in cases where there is a clear margin of separation between the classes.\n",
    "\n",
    "## How Does SVM Work?\n",
    "\n",
    "1. **Hyperplane**: In SVM, the idea is to find a hyperplane that best divides the dataset into classes. For 2D data, this hyperplane is simply a line, and in higher dimensions, it is a plane or a hyperplane.\n",
    "\n",
    "2. **Margin**: The margin is the distance between the hyperplane and the nearest data point from either class. SVM tries to maximize this margin to increase the classifier's robustness.\n",
    "\n",
    "3. **Support Vectors**: The data points that are closest to the hyperplane are called support vectors. These support vectors define the position of the hyperplane and are critical to the classifier's performance.\n",
    "\n",
    "4. **Kernel Trick**: When the data is not linearly separable, SVM uses a technique called the kernel trick. It maps the data points into a higher-dimensional space where they are linearly separable, allowing the SVM to find a hyperplane for classification.\n",
    "\n",
    "### Types of SVM\n",
    "\n",
    "- **Linear SVM**: Used when the data is linearly separable.\n",
    "- **Non-Linear SVM**: When the data is not linearly separable, SVM uses different kernels such as polynomial, Gaussian Radial Basis Function (RBF), and sigmoid to map the data into a higher-dimensional space.\n",
    "---\n",
    "## Key Terms in SVM\n",
    "\n",
    "1. **Support Vectors**: These are the data points that are closest to the decision boundary or hyperplane. These points are crucial as they influence the placement of the hyperplane.\n",
    "\n",
    "2. **Margin**: The margin is the distance between the support vectors and the decision boundary. SVM aims to maximize this margin for optimal classification.\n",
    "\n",
    "3. **Kernel Function**: A function used to map the input features into a higher-dimensional space where a linear decision boundary can be found. Popular kernel functions are:\n",
    "   - **Linear Kernel**: Used when the data is linearly separable.\n",
    "   - **Polynomial Kernel**: Used when the relationship between the features is polynomial.\n",
    "   - **RBF (Radial Basis Function) Kernel**: Used when there is a non-linear relationship between the features.\n",
    "   - **Sigmoid Kernel**: Used in some special cases, similar to the activation function in neural networks.\n",
    "---\n",
    "## Mathematical Formulation\n",
    "\n",
    "1. **Objective**: The objective of SVM is to find a hyperplane that maximizes the margin between the two classes.\n",
    "\n",
    "2. **Equation of Hyperplane**: The equation of the hyperplane is given by:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{w} \\cdot \\mathbf{x} + b = 0\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "   - $\\mathbf{w}$ is the weight vector normal to the hyperplane.\n",
    "   - $\\mathbf{x}$ is the input feature vector.\n",
    "   - $b$ is the bias term.\n",
    "\n",
    "3. **Maximizing the Margin**: The margin is given by:\n",
    "\n",
    "   $$\n",
    "   \\text{Margin} = \\frac{1}{\\|\\mathbf{w}\\|}\n",
    "   $$\n",
    "\n",
    "4. **Optimization Problem**: The optimization problem becomes:\n",
    "\n",
    "   $$\n",
    "   \\text{Minimize} \\quad \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "   $$\n",
    "\n",
    "   Subject to:\n",
    "\n",
    "   $$\n",
    "   y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\text{for each} \\quad i\n",
    "   $$\n",
    "\n",
    "   Where $y_i$ is the class label (+1 or -1) of the $i$-th sample.\n",
    "---\n",
    "## Advantages of SVM\n",
    "\n",
    "- **Effective in high-dimensional spaces**: Works well with datasets that have many features.\n",
    "- **Memory Efficient**: SVM is memory efficient as it only requires a subset of the training data (the support vectors) to build the classifier.\n",
    "- **Versatile**: SVM can handle both linear and non-linear classification problems using the kernel trick.\n",
    "- **Robust to Overfitting**: SVM is less prone to overfitting, especially in high-dimensional spaces.\n",
    "\n",
    "## Disadvantages of SVM\n",
    "\n",
    "- **Computationally Expensive**: SVM can be computationally expensive, especially with large datasets.\n",
    "- **Requires Tuning**: SVM requires careful tuning of parameters such as the regularization parameter (C) and the kernel function parameters.\n",
    "- **Not Ideal for Large Datasets**: SVMs do not scale well with large datasets, and the training time increases significantly as the dataset grows.\n",
    "\n",
    "## Applications of SVM\n",
    "\n",
    "- **Text Classification**: SVM is commonly used for document classification, such as spam detection and sentiment analysis.\n",
    "- **Image Classification**: SVM is effective in classifying images, especially in object recognition tasks.\n",
    "- **Bioinformatics**: SVM has been used in DNA sequence classification, protein structure prediction, and other bioinformatics applications.\n",
    "- **Face Detection**: SVM is used in face detection tasks to identify whether an image contains a human face.\n",
    "\n",
    "---\n",
    "Support Vector Machine (SVM) is a powerful algorithm used for classification tasks. It works well for both linear and non-linear classification problems. SVMâ€™s ability to maximize the margin between classes and its usage of the kernel trick make it a versatile algorithm for many machine learning applications. However, the algorithm can be computationally expensive and may require tuning to achieve the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc02d3d4-21cf-4970-a2e3-6be8ffc42158",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69485039-438a-49e9-8561-4f57c4d507ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a sample dataset (Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "data = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "data['target'] = y\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43417aea-7cfb-464c-90bc-9e204627df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the dimensions of the split data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d621241-2bf9-480f-bdb4-39b37871708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the SVM model with a linear kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Training the model with the training data\n",
    "svm_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ccc3-53dc-45f5-993a-0386f3711bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions using the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Displaying the predicted labels\n",
    "print(f\"Predicted Labels: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac73000-c923-4187-8876-f7220edf5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model performance using accuracy and classification report\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Displaying the classification report for detailed evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a5024-03a3-4d17-94b2-530cf82cbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For 2D data visualization (e.g., using only two features for simplicity)\n",
    "X_2d = X[:, :2]\n",
    "X_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the SVM model with a linear kernel\n",
    "svm_model_2d = SVC(kernel='linear')\n",
    "svm_model_2d.fit(X_train_2d, y_train)\n",
    "\n",
    "# Plotting the decision boundary\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Generate a grid of points to plot decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(X_train_2d[:, 0].min(), X_train_2d[:, 0].max(), 100),\n",
    "                     np.linspace(X_train_2d[:, 1].min(), X_train_2d[:, 1].max(), 100))\n",
    "\n",
    "# Predict class labels for each point in the grid\n",
    "Z = svm_model_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotting the decision boundaries\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "\n",
    "# Plotting the training points\n",
    "plt.scatter(X_train_2d[:, 0], X_train_2d[:, 1], c=y_train, edgecolors='k', marker='o')\n",
    "plt.title('SVM Decision Boundary (Linear Kernel)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
