{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e05f99b-ba4c-400d-b639-51cd22338f20",
   "metadata": {},
   "source": [
    "# ID3 Algorithm:-\n",
    "The **ID3 (Iterative Dichotomiser 3)** algorithm is a decision tree-based machine learning algorithm used for **classification tasks**. It was developed by Ross Quinlan in 1986 and is one of the foundational algorithms for constructing decision trees. The decision tree built by ID3 is used to classify data into distinct categories based on attribute values.\n",
    "\n",
    "### Key Concepts of ID3\n",
    "- ID3 constructs a decision tree by employing a **top-down, greedy search** through the given data.\n",
    "- It uses a mathematical concept called **Information Gain** (based on Entropy) to determine the best attribute to split the data at each step.\n",
    "- ID3 is primarily used for **categorical data** and doesn't directly handle numerical or continuous features.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is ID3 Used?\n",
    "\n",
    "- **Simplicity**: ID3 is simple to understand and implement, making it suitable for beginners.\n",
    "- **Transparency**: The resulting decision tree is interpretable, allowing users to see the reasoning behind predictions.\n",
    "- **Effectiveness for Small Data**: It works well for small to medium-sized datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does the ID3 Algorithm Work?\n",
    "\n",
    "1. **Start with the Root Node**:\n",
    "   - The entire dataset is used to calculate the **Entropy** (a measure of disorder) and **Information Gain** for all attributes.\n",
    "   - The attribute with the highest Information Gain is chosen as the **splitting criterion** for the root node.\n",
    "\n",
    "2. **Recursive Splitting**:\n",
    "   - The dataset is split into subsets based on the selected attribute's values.\n",
    "   - The process repeats for each subset, calculating the Entropy and Information Gain for remaining attributes.\n",
    "\n",
    "3. **Stopping Condition**:\n",
    "   - The recursion stops when one of the following is true:\n",
    "     - All examples in a subset belong to a single class.\n",
    "     - No attributes are left to split further.\n",
    "     - The dataset is empty.\n",
    "\n",
    "4. **Assign Class Labels**:\n",
    "   - Leaf nodes are labeled with the most frequent class in their subset.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematics Behind ID3\n",
    "### Entropy:\n",
    "Entropy is a measure of uncertainty or randomness in the data. For a binary classification problem, the formula for entropy is:\n",
    "\n",
    "$$\n",
    "\\text{Entropy}(S) = -p_+ \\cdot \\log_2(p_+) - p_- \\cdot \\log_2(p_-)\n",
    "$$\n",
    "\n",
    "where \\( p_+ \\) and \\( p_- \\) are the proportions of positive and negative examples in the dataset \\( S \\).\n",
    "\n",
    "### Information Gain:\n",
    "Information Gain measures the reduction in entropy after splitting on an attribute. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Gain}(A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v)\n",
    "$$\n",
    "\n",
    "Here, \\( S_v \\) is the subset of \\( S \\) for which attribute \\( A \\) has value \\( v \\).\n",
    "\n",
    "### Choosing the Attribute:\n",
    "The attribute with the highest Information Gain is selected for splitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages of ID3\n",
    "- Easy to implement and interpret.\n",
    "- Handles categorical data well.\n",
    "- Builds a simple and interpretable model.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages of ID3\n",
    "- Prone to **overfitting** if the tree becomes too deep.\n",
    "- Doesn't handle continuous or numerical attributes directly (requires preprocessing like discretization).\n",
    "- Cannot deal with missing values effectively.\n",
    "- Sensitive to noise in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## Real-World Applications\n",
    "1. **Email Spam Detection**:\n",
    "   - Classifying emails as spam or not based on keywords, sender, etc.\n",
    "2. **Medical Diagnosis**:\n",
    "   - Predicting diseases based on patient symptoms.\n",
    "3. **Customer Segmentation**:\n",
    "   - Categorizing customers based on buying behavior and demographics.\n",
    "4. **Loan Approval**:\n",
    "   - Classifying loan applications into approved or rejected based on applicant attributes.\n",
    "\n",
    "---\n",
    "\n",
    "The ID3 algorithm is widely used for creating decision trees for classification tasks. Itâ€™s simple to implement, interpretable, and useful for problems with categorical data. However, it has limitations, such as overfitting and difficulty handling continuous data, which newer algorithms like C4.5 and CART address.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "338351be-94df-4b3b-b4ab-3314539b84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "\n",
    "import math\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83a96e08-8511-47a0-922e-646ec0ab2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Calculate Entropy\n",
    "\n",
    "def entropy(data):\n",
    "    total = len(data)\n",
    "    counts = Counter([label for _, label in data])\n",
    "    return -sum((count / total) * math.log2(count / total) for count in counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc785964-ed93-4ede-b668-8dadf9e9cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Information Gain\n",
    "def information_gain(data, feature_index):\n",
    "    total_entropy = entropy(data)\n",
    "    total = len(data)\n",
    "    \n",
    "    # Group the data by the feature values\n",
    "    values = set([record[0][feature_index] for record in data])\n",
    "    \n",
    "    # Calculate weighted entropy\n",
    "    weighted_entropy = 0\n",
    "    for value in values:\n",
    "        subset = [record for record in data if record[0][feature_index] == value]\n",
    "        weighted_entropy += (len(subset) / total) * entropy(subset)\n",
    "    \n",
    "    return total_entropy - weighted_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "311d2176-594d-4b35-b9c2-55a8fa6e385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Choose Best Feature to Split\n",
    "def best_feature(data, features):\n",
    "    gains = [information_gain(data, feature) for feature in range(len(features))]\n",
    "    return gains.index(max(gains))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ea70fcb-2de7-4e2e-817f-dbb0529a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build the Decision Tree\n",
    "def build_tree(data, features):\n",
    "    # Base case 1: If all labels are the same\n",
    "    labels = [label for _, label in data]\n",
    "    if len(set(labels)) == 1:\n",
    "        return labels[0]\n",
    "    \n",
    "    # Base case 2: If no more features to split on\n",
    "    if not features:\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "    # Choose the best feature to split on\n",
    "    best = best_feature(data, features)\n",
    "    tree = {features[best]: {}}\n",
    "    \n",
    "    # Split the data based on the best feature\n",
    "    values = set([record[0][best] for record in data])\n",
    "    for value in values:\n",
    "        subset = [record for record in data if record[0][best] == value]\n",
    "        subtree = build_tree(subset, [f for i, f in enumerate(features) if i != best])\n",
    "        tree[features[best]][value] = subtree\n",
    "    \n",
    "    return tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "385e1e99-5cac-4e98-a9c6-667e20f6b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Predict with the Decision Tree\n",
    "def predict(tree, record, features):\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    \n",
    "    feature = list(tree.keys())[0]\n",
    "    value = record[features.index(feature)]\n",
    "    \n",
    "    # Check if the value is in the tree; if not, return the most common label\n",
    "    if value not in tree[feature]:\n",
    "        # Return the majority class (the most common label) in the data\n",
    "        labels = [label for _, label in data]\n",
    "        return Counter(labels).most_common(1)[0][0]\n",
    "    \n",
    "    return predict(tree[feature].get(value), record, features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dbaaec6-3745-469f-afe4-41ba97b181c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "# Define the data (each record is a tuple of (features, label))\n",
    "data = [\n",
    "    (['Sunny', 'Hot', 'High', 'Weak'], 'No'),\n",
    "    (['Sunny', 'Hot', 'High', 'Strong'], 'No'),\n",
    "    (['Overcast', 'Hot', 'High', 'Weak'], 'Yes'),\n",
    "    (['Rain', 'Mild', 'High', 'Weak'], 'Yes'),\n",
    "    (['Rain', 'Cool', 'Normal', 'Weak'], 'Yes'),\n",
    "    (['Rain', 'Cool', 'Normal', 'Strong'], 'No'),\n",
    "    (['Overcast', 'Cool', 'Normal', 'Strong'], 'Yes'),\n",
    "    (['Sunny', 'Mild', 'High', 'Weak'], 'No'),\n",
    "    (['Sunny', 'Cool', 'Normal', 'Weak'], 'Yes'),\n",
    "    (['Rain', 'Mild', 'Normal', 'Weak'], 'Yes'),\n",
    "    (['Sunny', 'Mild', 'Normal', 'Strong'], 'Yes'),\n",
    "    (['Overcast', 'Mild', 'High', 'Strong'], 'Yes'),\n",
    "    (['Overcast', 'Hot', 'Normal', 'Weak'], 'Yes'),\n",
    "    (['Rain', 'Mild', 'High', 'Strong'], 'No')\n",
    "]\n",
    "\n",
    "features = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab9b8024-03b3-4d23-a6ea-265de64c800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for ['Sunny', 'Cool', 'High', 'Strong'] : Yes\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Decision Tree\n",
    "tree = build_tree(data, features)\n",
    "# Step 7: Make Predictions\n",
    "test_record = ['Sunny', 'Cool', 'High', 'Strong']\n",
    "prediction = predict(tree, test_record, features)\n",
    "\n",
    "# Output the result\n",
    "print(\"Prediction for\", test_record, \":\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496990b-e83c-4c44-bffb-381b0e38f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deb9349f-eb53-4fd4-b6c1-59473e8a41cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "531039b3-3f82-4d9f-99e2-b40b7afeeaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ccd2a-3bce-4f0f-be54-ecda20a1cb1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e28d77-c43f-4b6d-8a81-1c031b00bedb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
